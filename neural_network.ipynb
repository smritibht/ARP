{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense # Fill missing values using linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = pd.read_parquet('data/Cleaned data/all_vehicles_electric__df.parquet', engine='pyarrow')\n",
    "ev_df = ev_df.interpolate() \n",
    "\n",
    "# replace Caithness and Sutherland and Ross and Cromarty with \"Caithness, Scotland\" in ITL level 3\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Caithness and Sutherland and Ross and Cromarty\"], \"Caithness, Scotland\")\n",
    "# replace Clackmannanshire and Fife with \"Clackmannanshire, Scotland\" in ITL level 3\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Clackmannanshire and Fife\"], \"Clackmannanshire, Scotland\")\n",
    "# replace East Ayrshire and North Ayrshire mainland with East Ayrshire\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"East Ayrshire and North Ayrshire mainland\"], \"East Ayrshire\")\n",
    "# replace East Dunbartonshire and West Dunbartonshire and Helensburgh and Lomond with East Dunbartonshire\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"East Dunbartonshire, West Dunbartonshire and Helensburgh and Lomond\"], \"East Dunbartonshire\")\n",
    "# replace East Lothian and Midlothian with East Lothian\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"East Lothian and Midlothian\"], \"East Lothian\")\n",
    "# replace Inverclyde, East Renfrewshire and Renfrewshire with Inverclyde\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Inverclyde, East Renfrewshire and Renfrewshire\"], \"Inverclyde\")\n",
    "# replace Inverness and Nairn and Moray, Badenoch and Strathspey with Inverness\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Inverness and Nairn and Moray, Badenoch and Strathspey\"], \"Inverness\")\n",
    "# replace Lochaber, Skye and Lochalsh, Arran and Cumbrae and Argyll and Bute with Lochaber\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Lochaber, Skye and Lochalsh, Arran and Cumbrae and Argyll and Bute\"], \"Lochaber\")\n",
    "# replace Armagh City, Banbridge and Craigavon with Armagh City\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Armagh City, Banbridge and Craigavon\"], \"Armagh City\")\n",
    "# replace Lisburn and Castlereagh with Lisburn\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Lisburn and Castlereagh\"], \"Lisburn\")\n",
    "# replace Mid and East Antrim with Mid Antrim\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Mid and East Antrim\"], \"Mid Antrim\")\n",
    "# replace Harrow and Hillingdon with Harrow\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Harrow and Hillingdon\"], \"Harrow\")\n",
    "# replace Merton, Kingston upon Thames and Sutton with Merton\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Merton, Kingston upon Thames and Sutton\"], \"Merton\")\n",
    "# replace Redbridge and Waltham Forest with Waltham Forest\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Redbridge and Waltham Forest\"], \"Waltham Forest\")\n",
    "# replace Barnsley, Doncaster and Rotherham with Barnsley\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Barnsley, Doncaster and Rotherham\"], \"Barnsley\")\n",
    "# replace Bath and North East Somerset, North Somerset and South Gloucestershire with North East Somerset\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Bath and North East Somerset, North Somerset and South Gloucestershire\"], \"North East Somerset\")\n",
    "# replace Bristol, City of with Bristol\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Bristol, City of\"], \"Bristol\")\n",
    "# replace Cornwall and Isles of Scilly with Cornwall\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Cornwall and Isles of Scilly\"], \"Cornwall\")\n",
    "# replace Breckland and South Norfolk with Breckland\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Breckland and South Norfolk\"], \"Breckland\")\n",
    "# replace Essex Haven Gateway with Essex\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Essex Haven Gateway\"], \"Essex\")\n",
    "# replace Essex Thames Gateway with Thames Gateway\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Essex Thames Gateway\"], \"Thames Gateway\")\n",
    "# replace North and West Norfolk with North Norfolk\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"North and West Norfolk\"], \"North Norfolk\")\n",
    "# replace Norwich and East Norfolk with Norwich\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Norwich and East Norfolk\"], \"Norwich\")\n",
    "# replace Kent Thames Gateway with Kent\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Kent Thames Gateway\"], \"Kent\")\n",
    "# replace Chorley and West Lancashire with Chorley\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Chorley and West Lancashire\"], \"Chorley\")\n",
    "# replace Conwy and Denbighshire with Conwy\n",
    "ev_df[\"ITL level 3\"] = ev_df[\"ITL level 3\"].replace([\"Conwy and Denbighshire\"], \"Conwy\")\n",
    "\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_latitude_longitude(place_name, max_retries=3):\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            location = geolocator.geocode(place_name)\n",
    "            if location:\n",
    "                latitude = location.latitude\n",
    "                longitude = location.longitude\n",
    "                return latitude, longitude\n",
    "            else:\n",
    "                return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Attempt {retries} failed. Retrying...\")\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n",
    "\n",
    "    print(\"Max retries exceeded. Could not fetch latitude and longitude.\")\n",
    "    return None, None\n",
    "\n",
    "# create a new dataframe with ITL level 3 unique values\n",
    "itl3_df = pd.DataFrame(ev_df[\"ITL level 3\"].unique(), columns=[\"ITL level 3\"])\n",
    "\n",
    "# get latitude and longitude for all values in the dataframe\n",
    "itl3_df[\"latitude\"], itl3_df[\"longitude\"] = zip(*itl3_df[\"ITL level 3\"].apply(get_latitude_longitude))\n",
    "# use the dataframe to create a dictionary for latitude and longitude\n",
    "itl3_dict = itl3_df.set_index('ITL level 3').T.to_dict('list')\n",
    "\n",
    "# add latitude and longitude to the original dataframe\n",
    "ev_df[\"latitude\"] = ev_df[\"ITL level 3\"].map(lambda x: itl3_dict[x][0])\n",
    "ev_df[\"longitude\"] = ev_df[\"ITL level 3\"].map(lambda x: itl3_dict[x][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a csv file\n",
    "ev_df.to_csv(\"data/Cleaned data/ev_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop columns that start with ITL\n",
    "ev_df = ev_df.loc[:,~ev_df.columns.str.startswith('ITL')]\n",
    "# drop columns that have ITL level 3, ITL level 1, ITL level 2, \n",
    "# Prepare input and target sequences\n",
    "input_sequence = ev_df.drop(columns=['Number of vehicles']).values\n",
    "target_sequence = ev_df[['Number of vehicles']].values\n",
    "\n",
    "\n",
    "# Sliding window technique to create input sequences\n",
    "window_size = len(input_sequence)\n",
    "encoder_input_data = []\n",
    "for i in range(len(input_sequence) - window_size + 1):\n",
    "    encoder_input_data.append(input_sequence[i : window_size])\n",
    "\n",
    "# Convert to numpy array\n",
    "encoder_input_data = np.array(encoder_input_data)\n",
    "\n",
    "# Prepare decoder input and target data\n",
    "decoder_input_data = target_sequence[0:window_size]\n",
    "decoder_target_data = target_sequence[:window_size]\n",
    "\n",
    "# Reshape decoder input and target data\n",
    "decoder_input_data = decoder_input_data.reshape(1, -1, 1)\n",
    "decoder_target_data = decoder_target_data.reshape(1, -1, 1)\n",
    "\n",
    "# Define the model\n",
    "encoder_inputs = Input(shape=(None, input_sequence.shape[1]))\n",
    "encoder = LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, target_sequence.shape[1]))\n",
    "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(target_sequence.shape[1], activation='sigmoid')  # Output is a sequence of the same shape as the target sequence\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, epochs=5)\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_sequence = model.predict([encoder_input_data, decoder_input_data])\n",
    "\n",
    "# Denormalize the predicted sequence (optional if normalization was applied earlier)\n",
    "predicted_sequence = predicted_sequence * target_sequence.max()\n",
    "\n",
    "# Print the predicted sequence\n",
    "print(predicted_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df.dtypes\n",
    "\n",
    "\n",
    "# Assuming 'ev_df' is your DataFrame containing both numeric and non-numeric columns\n",
    "\n",
    "# Select only numeric columns (both integer and float)\n",
    "ev_df_numeric = ev_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# 'ev_df_numeric' will now contain only the columns that are either integer or float.\n",
    "ev_df_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Prepare input and target sequences\n",
    "input_sequence = ev_df_numeric.drop(columns=['Number of vehicles']).values\n",
    "target_sequence = ev_df_numeric[['Number of vehicles']].values\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "input_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))  # Set feature_range to handle negative values\n",
    "input_sequence = input_scaler.fit_transform(input_sequence)\n",
    "target_sequence = target_scaler.fit_transform(target_sequence)\n",
    "\n",
    "# Padding input_sequence and target_sequence\n",
    "max_sequence_length = 15  # Choose an appropriate value based on the dataset\n",
    "input_sequence = np.pad(input_sequence, ((0, 0), (0, max_sequence_length - input_sequence.shape[1])))\n",
    "target_sequence = np.pad(target_sequence, ((0, 0), (0, max_sequence_length - target_sequence.shape[1])))\n",
    "\n",
    "# Reshape the target_sequence to match decoder input shape\n",
    "decoder_input_data = target_sequence[:, :-1].reshape(1, -1, 1)\n",
    "decoder_target_data = target_sequence[:, 1:].reshape(1, -1, 1)\n",
    "\n",
    "# Define the model\n",
    "encoder_inputs = Input(shape=(None, input_sequence.shape[1]))\n",
    "encoder = LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, 1))\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(1, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with batching\n",
    "batch_size = 32  # Choose an appropriate batch size based on the dataset\n",
    "model.fit([input_sequence[np.newaxis, :], decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size, epochs=2)\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_sequence = model.predict([input_sequence[np.newaxis, :], decoder_input_data])\n",
    "\n",
    "# Inverse transform the predicted sequence to obtain the actual values\n",
    "predicted_sequence = target_scaler.inverse_transform(predicted_sequence[0])\n",
    "\n",
    "# Print the predicted sequence\n",
    "print(predicted_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Flatten the predicted and actual sequences\n",
    "predicted_sequence_flattened = predicted_sequence.flatten()\n",
    "\n",
    "\n",
    "# Extract the actual target sequence from 'ev_df'\n",
    "actual_sequence = ev_df['Number of vehicles'].values\n",
    "# Reshape the actual_sequence to match the shape of the predicted_sequence\n",
    "actual_sequence = actual_sequence.reshape(1, -1, 1)\n",
    "# Denormalize the actual_sequence (optional if normalization was applied earlier)\n",
    "actual_sequence = actual_sequence * target_sequence.max()\n",
    "\n",
    "actual_sequence_flattened = actual_sequence.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_sequence_flattened, predicted_sequence_flattened)\n",
    "mae = mean_absolute_error(actual_sequence_flattened, predicted_sequence_flattened)\n",
    "r2 = r2_score(actual_sequence_flattened, predicted_sequence_flattened)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used in the provided code is a sequence-to-sequence (seq2seq) model with an LSTM (Long Short-Term Memory) architecture. The seq2seq model is a type of recurrent neural network (RNN) model that is commonly used for sequence prediction tasks, such as time series forecasting and natural language translation.\n",
    "\n",
    "The seq2seq model consists of two main parts: an encoder and a decoder. The encoder processes the input sequence (in this case, spatial data like \"latitude\" and \"longitude\") and encodes it into a fixed-length representation, often called the context or hidden state. The decoder then takes this context vector as input and generates the output sequence (in this case, predicting the \"Number of vehicles\").\n",
    "\n",
    "Here's a breakdown of the model components:\n",
    "\n",
    "1. Encoder:\n",
    "- Input: The spatial data (latitude and longitude) as input sequences.\n",
    "- LSTM Layer: A Long Short-Term Memory (LSTM) layer with 128 units. This LSTM layer processes the input sequences and captures the temporal dependencies in the spatial data. The LSTM layer returns the final hidden state and cell state of the encoder, which together form the context vector.\n",
    "\n",
    "2. Decoder:\n",
    "- Input: The target sequence (\"Number of vehicles\") as input sequences.\n",
    "- LSTM Layer: Another LSTM layer with 128 units. This LSTM layer processes the target sequence and generates sequences of hidden states.\n",
    "- Dense Layer: A dense (fully connected) layer with the same number of output nodes as the target sequence. This layer applies a linear activation function to produce the predicted output sequence.\n",
    "\n",
    "The model is trained using the Adam optimizer and mean squared error (MSE) as the loss function.\n",
    "\n",
    "The seq2seq LSTM model is commonly used for time series forecasting and other sequence-to-sequence tasks because it can handle variable-length input and output sequences and capture complex temporal patterns. However, the performance of the model heavily depends on the quality and representation of the input data, as well as the hyperparameters and architecture chosen during model development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
